{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from bs4 import BeautifulSoup\n",
    "import concurrent.futures\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "\n",
    "# [Keep all helper functions (get_summary_value, get_ld_data, get_data) unchanged] \n",
    "def get_summary_value(card, data_summary):\n",
    "    element = card.select_one(f\"[data-summary='{data_summary}']\")\n",
    "    if element:\n",
    "        text = element.get_text(strip=True)\n",
    "        if not text and element.next_sibling:\n",
    "            text = element.next_sibling.strip() if isinstance(element.next_sibling, str) else \"\"\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "def get_ld_data(card, idx):\n",
    "    ld_json_data = {}\n",
    "    ld_script = card.find(\"script\", type=\"application/ld+json\")\n",
    "    if ld_script and ld_script.string:\n",
    "        try:\n",
    "            ld_json_data = json.loads(ld_script.string)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing ld+json for property {idx}: {e}\")\n",
    "    return ld_json_data\n",
    "\n",
    "def get_data(soup):\n",
    "    possible_classes = ['mb-srp__card', 'mb-srp_list', 'mb-srpCard', 'srpCard', 'mb-srp']\n",
    "    css_selector = ', '.join([f'div.{cls}' for cls in possible_classes])\n",
    "    cards = soup.select(css_selector)\n",
    "    \n",
    "    if not cards:\n",
    "        print(\"No cards found with known class names.\")\n",
    "        return []\n",
    "    \n",
    "    properties = []\n",
    "    for idx, card in enumerate(cards, 1):\n",
    "        try:\n",
    "            title = ''\n",
    "            for title_class in ['mb-srp__card--title', 'mb-srp_card--title']:\n",
    "                title_elem = card.find('h2', class_=title_class)\n",
    "                if title_elem:\n",
    "                    title = title_elem.get('title', '').strip() or title_elem.get_text(strip=True)\n",
    "                    break\n",
    "            \n",
    "            price = ''\n",
    "            for price_class in ['mb-srp__card__price--amount', 'mb-srp_card_price--amount']:\n",
    "                price_elem = card.find('div', class_=price_class)\n",
    "                if price_elem:\n",
    "                    price = price_elem.get_text(strip=True)\n",
    "                    break\n",
    "            \n",
    "            area = get_summary_value(card, 'super-area') or get_summary_value(card, 'carpet-area')\n",
    "            transaction = get_summary_value(card, 'transaction')\n",
    "            furnishing  = get_summary_value(card, 'furnishing')\n",
    "            society     = get_summary_value(card, 'society')\n",
    "            bathroom    = get_summary_value(card, 'bathroom')\n",
    "            balcony     = get_summary_value(card, 'balcony')\n",
    "            \n",
    "            data = {\n",
    "                \"title\": title,\n",
    "                \"price\": price,\n",
    "                \"area\": area,\n",
    "                \"transaction\": transaction,\n",
    "                \"furnishing\": furnishing,\n",
    "                \"society\": society,\n",
    "                \"bathroom\": bathroom,\n",
    "                \"balcony\": balcony\n",
    "            }\n",
    "            \n",
    "            usp_items = card.select(\"div.mb-srp_card_usp--item\")\n",
    "            data[\"usp_details\"] = [item.get_text(strip=True) for item in usp_items] if usp_items else []\n",
    "            \n",
    "            ld_json_data = get_ld_data(card, idx)\n",
    "            if ld_json_data:\n",
    "                data[\"numberOfRooms\"] = ld_json_data.get(\"numberOfRooms\", \"\")\n",
    "                geo_data = ld_json_data.get(\"geo\", {})\n",
    "                data[\"latitude\"] = geo_data.get(\"latitude\", \"\")\n",
    "                data[\"longitude\"] = geo_data.get(\"longitude\", \"\")\n",
    "            \n",
    "            properties.append(data)\n",
    "            print(f\"Processed property {idx}: {title}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error on property {idx}: {str(e)}\")\n",
    "            continue\n",
    "    return properties\n",
    "\n",
    "def process_city_pages(city, start_page, end_page, retries=3):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2})\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    city_results = []\n",
    "    \n",
    "    try:\n",
    "        for page in range(start_page, end_page + 1):\n",
    "            url = (f\"https://www.magicbricks.com/property-for-sale/residential-real-estate?\"\n",
    "                   f\"proptype=Multistorey-Apartment,Builder-Floor-Apartment,Penthouse,Studio-Apartment,\"\n",
    "                   f\"Residential-House,Villa&page={page}&cityName={city}\")\n",
    "            \n",
    "            for attempt in range(retries):\n",
    "                try:\n",
    "                    print(f\"{city}: Page {page} (Attempt {attempt+1}/{retries})\")\n",
    "                    driver.get(url)\n",
    "                    WebDriverWait(driver, 15).until\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"div.mb-srp__card, div.m-srp_card\"))\n",
    "                    \n",
    "                    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                    props = get_data(soup)\n",
    "                    city_results.extend(props)\n",
    "                    print(f\"{city}: Page {page} → {len(props)} properties\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"{city} Page {page} error: {str(e)}\")\n",
    "                    time.sleep(2)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return city_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city_property_counts = {\n",
    "    \"Noida\": 12216,\n",
    "    \"Ghaziabad\": 8216,\n",
    "    \"Greater-Noida\": 8691,\n",
    "    \"Navi-Mumbai\": 12357,\n",
    "    \"Faridabad\": 3551,\n",
    "    \"Bhubaneswar\": 3819,\n",
    "    \"Bokaro-Steel-City\": 85,\n",
    "    \"Vijayawada\": 888,\n",
    "    \"Vrindavan\": 338,\n",
    "    \"Bhopal\": 1996,\n",
    "    \"Gorakhpur\": 154,\n",
    "    \"Jamshedpur\": 1057,\n",
    "    \"Agra\": 1170,\n",
    "    \"Allahabad\": 589,\n",
    "    \"Jodhpur\": 379,\n",
    "    \"Aurangabad\": 594,\n",
    "    \"Jaipur\": 10846,\n",
    "    \"Mangalore\": 785,\n",
    "    \"Nagpur\": 3502,\n",
    "    \"Guntur\": 407,\n",
    "    \"Navsari\": 90,\n",
    "    \"Palghar\": 470,\n",
    "    \"Salem\": 181,\n",
    "    \"Haridwar\": 428,\n",
    "    \"Durgapur\": 363,\n",
    "    \"Madurai\": 561,\n",
    "    \"Manipal\": 23,\n",
    "    \"Patna\": 1389,\n",
    "    \"Ranchi\": 1356,\n",
    "    \"Raipur\": 870,\n",
    "    \"Sonipat\": 616,\n",
    "    \"Kottayam\": 154,\n",
    "    \"Kozhikode\": 284,\n",
    "    \"Thrissur\": 733,\n",
    "    \"Tirupati\": 300,\n",
    "    \"Trivandrum\": 853,\n",
    "    \"Trichy\": 477,\n",
    "    \"Udaipur\": 373,\n",
    "    \"Vapi\": 179,\n",
    "    \"Varanasi\": 925,\n",
    "    \"Vadodara\": 4344,\n",
    "    \"Visakhapatnam\": 2805,\n",
    "    \"Surat\": 3556,\n",
    "    \"Kanpur\": 1531,\n",
    "    \"Kochi\": 1546,\n",
    "    \"Mysore\": 804,\n",
    "    \"Goa\": 2348,\n",
    "    \"Bhiwadi\": 884,\n",
    "    \"Lucknow\": 9189,\n",
    "    \"Nashik\": 1692,\n",
    "    \"Guwahati\": 1660,\n",
    "    \"Chandigarh\": 2920,\n",
    "    \"Indore\": 3525,\n",
    "    \"Coimbatore\": 4075,\n",
    "    \"Dehradun\": 2226\n",
    "}\n",
    "\n",
    "    all_properties = []\n",
    "    MAX_WORKERS = 6  # Reduced for 16GB RAM stability\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = {}\n",
    "        for city, count in city_property_counts.items():\n",
    "            end_page = math.ceil(count / 30)  # Calculate pages dynamically\n",
    "            futures[executor.submit(process_city_pages, city, 1, end_page)] = city\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            city = futures[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                all_properties.extend(data)\n",
    "                print(f\"✅ {city}: Collected {len(data)} properties\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ {city} failed: {str(e)}\")\n",
    "\n",
    "    print(f\"\\nTotal collected: {len(all_properties)} properties\")\n",
    "    \n",
    "    with open(\"properties.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_properties, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converted the json file into excel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define input JSON file and output Excel file\n",
    "json_file = \"new.json\"  # Change this to your actual file\n",
    "excel_file = \"output.xlsx\"\n",
    "\n",
    "# Step 1: Read and Fix JSON Format\n",
    "try:\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        raw_data = file.read()\n",
    "    \n",
    "    # Attempt to parse as a full JSON structure\n",
    "    try:\n",
    "        data = json.loads(raw_data)  # Load JSON\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"⚠ JSON format error detected! Trying line-by-line parsing...\")\n",
    "        data = []\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            for i, line in enumerate(file, 1):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue  # Skip empty lines\n",
    "                try:\n",
    "                    obj = json.loads(line)  # Parse each line\n",
    "                    data.append(obj)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"❌ Skipping line {i} due to JSON error: {e}\")\n",
    "        \n",
    "    # Ensure valid data was read\n",
    "    if not data:\n",
    "        raise ValueError(\"No valid JSON objects found!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ JSON file not found! Check the file path.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Unexpected error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Save the formatted JSON (optional)\n",
    "formatted_json_file = \"formatted_total.json\"\n",
    "with open(formatted_json_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "print(f\"✅ Formatted JSON saved to {formatted_json_file}\")\n",
    "\n",
    "# Step 3: Convert JSON to Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 4: Convert list-type columns (like 'usp_details') to strings\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].apply(lambda x: \", \".join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Step 5: Save to Excel\n",
    "df.to_excel(excel_file, index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(f\"✅ Successfully converted JSON to {excel_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
